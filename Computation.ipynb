{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba336a1e",
   "metadata": {},
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5716d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries cell\n",
    "\n",
    "import json\n",
    "import time\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    " \n",
    "from pyspark.sql           import functions as F\n",
    "from pyspark.sql           import SparkSession\n",
    "from pyspark.streaming     import StreamingContext\n",
    "from pyspark.sql.types     import StructField, StructType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, to_json, col, when, sum, count, struct, collect_list\n",
    "from pyspark               import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6badc",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0d2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of spark from the packages folder\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7557507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aidin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aidin/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c2a6b55a-b4b6-4cf2-908b-d8fd13936c9c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 725ms :: artifacts dl 60ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c2a6b55a-b4b6-4cf2-908b-d8fd13936c9c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/14ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/13 19:53:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pd-master:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_CosmicRays_Dashboard_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4ee9663bb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start session - specify port, application name, and configuration settings.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Project_CosmicRays_Dashboard_application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "        .config(\"spark.ui.port\", \"4041\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2e52c",
   "metadata": {},
   "source": [
    "## Kafka Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8523564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin = KafkaAdminClient(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bea9335e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'results', '__consumer_offsets']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70898183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='results', error_code=0)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.delete_topics(['results'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ddad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', '__consumer_offsets']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d644147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_topic = NewTopic(name='results', \n",
    "                       num_partitions=4, \n",
    "                       replication_factor=1)\n",
    "kafka_admin.create_topics(new_topics=[results_topic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d81a24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'results', '__consumer_offsets']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447ad826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the input dataframe and its source. Define subscription to 'stream' - one of the two topics in kafka\n",
    "inputDF = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'stream')\\\n",
    "        .load()\n",
    "\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0fa0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the schema of the rows that will be read. double are used to overcome overflow issues\n",
    "schema = StructType(\n",
    "        [\n",
    "            StructField(\"HEAD\",        IntegerType()),\n",
    "            StructField(\"FPGA\",        IntegerType()),\n",
    "            StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "            StructField(\"ORBIT_CNT\",    DoubleType()),\n",
    "            StructField(\"BX_COUNTER\",   DoubleType()), \n",
    "            StructField(\"TDC_MEAS\",     DoubleType())\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408c15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: integer (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: double (nullable = true)\n",
      " |    |-- BX_COUNTER: double (nullable = true)\n",
      " |    |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert input_Df to json by casting columns into the predefined schema.\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))\n",
    "\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "348f6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: double (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#flattening the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")\n",
    "\n",
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ee8ea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: double (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      " |-- CHAMBER: integer (nullable = true)\n",
      " |-- ABSOLUTETIME: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data-cleansing, removing ancillary hits\n",
    "# division of dataframe between chambers\n",
    "# adding absolute time column to define DRIFTIME\n",
    "df = flatDF.filter(col('HEAD') == 2)\\\n",
    "    .withColumn('CHAMBER',\n",
    "                when(( col( 'FPGA' ) == 0 ) & ( col( 'TDC_CHANNEL' ) <= 63 ),                                   0) \\\n",
    "               .when(( col( 'FPGA' ) == 0 ) & ( col( 'TDC_CHANNEL' ) >  63 ) & ( col( 'TDC_CHANNEL' ) <= 127 ), 1) \\\n",
    "               .when(( col( 'FPGA' ) == 1 ) & ( col( 'TDC_CHANNEL' ) <= 63 ),                                   2) \\\n",
    "               .when(( col( 'FPGA' ) == 1 ) & ( col( 'TDC_CHANNEL' ) >  63 ) & ( col( 'TDC_CHANNEL' ) <= 127 ), 3) \\\n",
    "               .when(( col( 'FPGA' ) == 1 ) &                                  ( col( 'TDC_CHANNEL' ) == 128 ) ,4) \\\n",
    "               .otherwise(None))                                                                                   \\\n",
    "    .filter( col( 'CHAMBER' ).isNotNull() )                                                                        \\\n",
    "    .withColumn('ABSOLUTETIME', 25 * ( col( 'ORBIT_CNT' ) * 3564 + col( 'BX_COUNTER' ) + col( 'TDC_MEAS' ) / 30 ) )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4d971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "144ff0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_json = {\n",
    "    'epoch_id' : 0,\n",
    "    'hits'     : 0,\n",
    "    'CH0' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'DRIFTIME'    : []\n",
    "            },\n",
    "    'CH1' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'DRIFTIME'    : []\n",
    "            },\n",
    "    'CH2' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'DRIFTIME'    : []\n",
    "            },\n",
    "    'CH3' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'DRIFTIME'    : []\n",
    "            },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47ddd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(df, epoch_id):   \n",
    "\n",
    "    '''\n",
    "    Function for the batch processing of the data.\n",
    "    The processing consists in retrieving\n",
    "    the following informations:\n",
    "        1. total number of processed hits, \n",
    "           post-clensing (1 value per batch)\n",
    "        2. total number of processed hits,\n",
    "           post-clensing, per chamber (4 values per batch)\n",
    "        3. histogram of the counts of active TDC_CHANNEL,\n",
    "           per chamber (4 arrays per batch)\n",
    "        4. histogram of the total number of active\n",
    "           TDC_CHANNEL in each ORBIT_CNT, per chamber \n",
    "           (4 arrays per batch)\n",
    "        5. histogram of the counts of active TDC_CHANNEL,\n",
    "           per chamber, ONLY for those orbits with at least\n",
    "           one scintillator signal in it (4 arrays per batch)\n",
    "        6. histogram of the DRIFTIME, per chamber \n",
    "           (4 arrays per batch)\n",
    "    \n",
    "    Inputs:\n",
    "        - df: spark dataframe with the data\n",
    "        - epoch_id: batch index    \n",
    "    '''\n",
    "\n",
    "    msg_json['CH0']['hits'] = 0\n",
    "    msg_json['CH1']['hits'] = 0\n",
    "    msg_json['CH2']['hits'] = 0\n",
    "    msg_json['CH3']['hits'] = 0\n",
    "    msg_json['epoch_id']    = epoch_id\n",
    "\n",
    "\n",
    "    # total number of processed hits\n",
    "    # post-cleansing\n",
    "    total_hits = df.filter(col('CHAMBER') != 4)\\\n",
    "                   .count()\n",
    "    \n",
    "\n",
    "    # total number of processed hits,\n",
    "    # post-clensing, per chamber\n",
    "    df_counts = df.filter(col('CHAMBER') != 4) \\\n",
    "                  .groupBy(   'CHAMBER')       \\\n",
    "                  .count()                     \\\n",
    "                  .withColumnRenamed('count', 'ch_hits')\n",
    "\n",
    "\n",
    "    # histogram of the counts of active\n",
    "    # TDC_CHANNEL, per chamber.\n",
    "    df_counts_ch = df.filter(col('CHAMBER') != 4)            \\\n",
    "                     .groupBy('TDC_CHANNEL', 'CHAMBER')      \\\n",
    "                     .count()                                \\\n",
    "                     .groupBy('CHAMBER')                     \\\n",
    "                     .agg(struct(collect_list('TDC_CHANNEL') \\\n",
    "                                       .alias('bin_edges'),\n",
    "                                 collect_list(    'count')   \\\n",
    "                                       .alias(   'counts'),\n",
    "                                )                            \\\n",
    "                          .alias('histo_CH')\n",
    "                         )\n",
    "    \n",
    "    \n",
    "    # histogram of the total number of active\n",
    "    # TDC_CHANNEL in each ORBIT_CNT per chamber\n",
    "    \n",
    "    df_counts_orb = df.filter(col('CHAMBER') != 4)            \\\n",
    "                      .groupBy('ORBIT_CNT', 'CHAMBER')        \\\n",
    "                      .agg(F.countDistinct('TDC_CHANNEL'))    \\\n",
    "                      .groupBy(col('count(TDC_CHANNEL)'))     \\\n",
    "                      .count()                                \\\n",
    "                      .groupBy('CHAMBER')                     \\\n",
    "                      .agg(struct(collect_list('TDC_CHANNEL') \\\n",
    "                                        .alias(  'bin_edges'),\n",
    "                                  collect_list(      'count') \\\n",
    "                                        .alias(     'counts'),\n",
    "                                 )                            \\\n",
    "                           .alias('histo_ORB')\n",
    "                          )\n",
    "\n",
    "\n",
    "    # Histogram of the count of active TDC_CHANNEL,\n",
    "    # per chamber, only for those orbits with\n",
    "    # at least one scintillatorin it\n",
    "\n",
    "    df_scint = df.select('CHAMBER', 'ORBIT_CNT', 'ABSOLUTETIME') \\\n",
    "                     .filter(col('CHAMBER') == 4)                    \\\n",
    "                     .groupBy('ORBIT_CNT')                           \\\n",
    "                     .min('ABSOLUTETIME')                            \\\n",
    "                     .withColumnRenamed('min(ABSOLUTETIME)', 't0')\n",
    "    list_scint = df_sc['ORBIT_CNT'].tolist()\n",
    "    \n",
    "    \n",
    "    df_sc = df.filter(col('CHAMBER') != 4 )                   \\\n",
    "              .where( col( 'ORBIT_CNT' ).isin( list_scint ) ) \\\n",
    "              .groupBy(  'TDC_CHANNEL', 'CHAMBER'       )     \\\n",
    "              .count()                                        \\\n",
    "              .agg(struct(collect_list('TDC_CHANNEL')         \\\n",
    "                                  .alias('bin_edges'),\n",
    "                            collect_list(    'count')         \\\n",
    "                                  .alias(   'counts'),\n",
    "                         ).alias('histo_SC')\n",
    "                  )\n",
    " \n",
    "    \n",
    "    # histogram of the DRIFTIME, per chamber (4 arrays per batch)\n",
    "    df_drift = df.select('ORBIT_CNT', 'CHAMBER', 'ABSOLUTETIME') \\\n",
    "                 .filter(col('chamber') != 4)                  \\\n",
    "                 .join(df_scint, on='ORBIT_CNT', how='rightouter')\\\n",
    "                 .withColumn('DRIFTIME',\n",
    "                           when(col('CHAMBER') == 0, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[0]))\n",
    "                          .when(col('CHAMBER') == 1, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[1]))\n",
    "                          .when(col('CHAMBER') == 2, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[2]))\n",
    "                          .when(col('CHAMBER') == 3, col('ABSOLUTETIME') - (col('t0') - time_offset_by_chamber[3]))\n",
    "                           )\\\n",
    "                .groupBy('CHAMBER')\\\n",
    "                .agg(collect_list('DRIFTIME').alias('DRIFTIME'))\n",
    "\n",
    "    msg_json['hits'] = total_hits\n",
    "    for i, ch in enumerate(df_counts['CHAMBER']):\n",
    "        msg_json[f'CH{chamber}']['total_hits']             = df_counts[    ch]['ch_hits']\n",
    "        msg_json[f'CH{chamber}']['hist_CH'][  'bin_edges'] = df_counts_ch[ ch]['hist_CHANNEL']['bin_edges']\n",
    "        msg_json[f'CH{chamber}']['hist_CH'][' bin_counts'] = df_counts_ch[ ch]['hist_CHANNEL']['counts']\n",
    "        msg_json[f'CH{chamber}']['hist_ORB'][ 'bin_edges'] = df_counts_orb[ch]['hist_ORBIT']['bin_edges']\n",
    "        msg_json[f'CH{chamber}']['hist_ORB']['bin_counts'] = df_counts_orb[ch]['hist_ORBIT']['counts']\n",
    "        msg_json[f'CH{chamber}']['hist_SC'][  'bin_edges'] = df_sc[        ch]['hist_SCINT']['bin_edges']\n",
    "        msg_json[f'CH{chamber}']['hist_SC'][ 'bin_counts'] = df_sc[        ch]['hist_SCINT']['counts']\n",
    "        msg_json[f'CH{chamber}']['DRIFTIME']               = df_drift[     ch]['DRIFTIME']\n",
    "\n",
    "    producer.send('results', json.dumps(msg_json).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4de2c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d920458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/13 22:41:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cadfb525-7e2e-4e4f-ac13-9545bb7349ce. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/09/13 22:41:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/09/13 22:41:54 ERROR MicroBatchExecution: Query [id = 90d65950-a64b-4020-96e4-72252a501ee6, runId = 6424efc8-399d-4bd5-a802-fc97391c3f67] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_4533/621440559.py\", line 36, in batch_proc\n",
      "    total_hits = df.filter(col('CHAMBER') != 4)\\\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 2079, in filter\n",
      "    jdf = self._jdf.filter(condition._jc)\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: Column 'CHAMBER' does not exist. Did you mean one of the following? [HEAD, BX_COUNTER, FPGA, TDC_CHANNEL, TDC_MEAS, ORBIT_CNT];\n",
      "'Filter NOT ('CHAMBER = 4)\n",
      "+- LogicalRDD [HEAD#112, FPGA#113, TDC_CHANNEL#114, ORBIT_CNT#115, BX_COUNTER#116, TDC_MEAS#117], false\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy31.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = 90d65950-a64b-4020-96e4-72252a501ee6, runId = 6424efc8-399d-4bd5-a802-fc97391c3f67] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 272, in call\n    raise e\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 269, in call\n    self.func(DataFrame(jdf, self.session), batch_id)\n  File \"/tmp/ipykernel_4533/621440559.py\", line 36, in batch_proc\n    total_hits = df.filter(col('CHAMBER') != 4)\\\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 2079, in filter\n    jdf = self._jdf.filter(condition._jc)\n  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Column 'CHAMBER' does not exist. Did you mean one of the following? [HEAD, BX_COUNTER, FPGA, TDC_CHANNEL, TDC_MEAS, ORBIT_CNT];\n'Filter NOT ('CHAMBER = 4)\n+- LogicalRDD [HEAD#112, FPGA#113, TDC_CHANNEL#114, ORBIT_CNT#115, BX_COUNTER#116, TDC_MEAS#117], false\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mflatDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_proc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Query [id = 90d65950-a64b-4020-96e4-72252a501ee6, runId = 6424efc8-399d-4bd5-a802-fc97391c3f67] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 272, in call\n    raise e\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 269, in call\n    self.func(DataFrame(jdf, self.session), batch_id)\n  File \"/tmp/ipykernel_4533/621440559.py\", line 36, in batch_proc\n    total_hits = df.filter(col('CHAMBER') != 4)\\\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 2079, in filter\n    jdf = self._jdf.filter(condition._jc)\n  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/home/aidin/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Column 'CHAMBER' does not exist. Did you mean one of the following? [HEAD, BX_COUNTER, FPGA, TDC_CHANNEL, TDC_MEAS, ORBIT_CNT];\n'Filter NOT ('CHAMBER = 4)\n+- LogicalRDD [HEAD#112, FPGA#113, TDC_CHANNEL#114, ORBIT_CNT#115, BX_COUNTER#116, TDC_MEAS#117], false\n\n"
     ]
    }
   ],
   "source": [
    "flatDF.writeStream\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545eda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f4c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
