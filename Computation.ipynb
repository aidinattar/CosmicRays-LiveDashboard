{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba336a1e",
   "metadata": {},
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5716d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries cell\n",
    "\n",
    "import json\n",
    "import time\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from kafka                import KafkaProducer\n",
    "from kafka.admin          import KafkaAdminClient, NewTopic\n",
    " \n",
    "from pyspark.sql           import functions as F\n",
    "from pyspark.sql           import SparkSession\n",
    "from pyspark.streaming     import StreamingContext\n",
    "from pyspark.sql.types     import StructField, StructType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, when, count, struct, collect_list\n",
    "from pyspark               import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StringType "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6badc",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0d2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of spark from the packages folder\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7557507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aidin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aidin/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-78016658-3550-4fec-8ad4-c60673d88d72;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 847ms :: artifacts dl 30ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-78016658-3550-4fec-8ad4-c60673d88d72\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 20:38:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pd-master:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_CosmicRays_Dashboard_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f000a4095b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start session - specify port, application name, and configuration settings.\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Project_CosmicRays_Dashboard_application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "        .config(\"spark.ui.port\", \"4041\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2e52c",
   "metadata": {},
   "source": [
    "## Kafka Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8523564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "kafka_admin = KafkaAdminClient(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925729a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_stream', '__consumer_offsets', 'results', 'stream', 'topic_results']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea9335e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='stream', error_code=0), (topic='results', error_code=0)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.delete_topics(['stream', 'results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ddad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_stream', 'topic_results', '__consumer_offsets']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d644147",
   "metadata": {},
   "outputs": [
    {
     "ename": "TopicAlreadyExistsError",
     "evalue": "[Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='topic_results', num_partitions=4, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='topic_results', error_code=36, error_message=\"Topic 'topic_results' already exists.\")])'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m results_topic \u001b[38;5;241m=\u001b[39m NewTopic(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_results\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                        num_partitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[1;32m      3\u001b[0m                        replication_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mkafka_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mresults_topic\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/admin/client.py:461\u001b[0m, in \u001b[0;36mKafkaAdminClient.create_topics\u001b[0;34m(self, new_topics, timeout_ms, validate_only)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport for CreateTopics v\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has not yet been added to KafkaAdminClient.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(version))\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# TODO convert structs to a more pythonic interface\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# TODO raise exceptions if errors\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request_to_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/admin/client.py:407\u001b[0m, in \u001b[0;36mKafkaAdminClient._send_request_to_controller\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m error_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNoError:\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error_type(\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m failed with response \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m             \u001b[38;5;241m.\u001b[39mformat(request, response))\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m: [Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='topic_results', num_partitions=4, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='topic_results', error_code=36, error_message=\"Topic 'topic_results' already exists.\")])'."
     ]
    }
   ],
   "source": [
    "results_topic = NewTopic(name='topic_results', \n",
    "                       num_partitions=4, \n",
    "                       replication_factor=1)\n",
    "kafka_admin.create_topics(new_topics=[results_topic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d81a24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_stream', 'topic_results', '__consumer_offsets']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447ad826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input dataframe and its source. \n",
    "# Define subscription to 'topic_stream' - one of the two topics in kafka\n",
    "\n",
    "inputDF = spark.readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'topic_stream')\\\n",
    "        .load()\n",
    "\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0fa0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme of the imput data\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        StringType()),\n",
    "                StructField(\"FPGA\",        StringType()),\n",
    "                StructField(\"TDC_CHANNEL\", StringType()),\n",
    "                StructField(\"ORBIT_CNT\",   StringType()),\n",
    "                StructField(\"BX_COUNTER\",  StringType()),\n",
    "                StructField(\"TDC_MEAS\",    StringType())\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "408c15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: string (nullable = true)\n",
      " |    |-- FPGA: string (nullable = true)\n",
      " |    |-- TDC_CHANNEL: string (nullable = true)\n",
      " |    |-- ORBIT_CNT: string (nullable = true)\n",
      " |    |-- BX_COUNTER: string (nullable = true)\n",
      " |    |-- TDC_MEAS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert inputDF to jsonDF\n",
    "\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))\n",
    "\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "348f6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: string (nullable = true)\n",
      " |-- FPGA: string (nullable = true)\n",
      " |-- TDC_CHANNEL: string (nullable = true)\n",
      " |-- ORBIT_CNT: string (nullable = true)\n",
      " |-- BX_COUNTER: string (nullable = true)\n",
      " |-- TDC_MEAS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatDF = jsonDF.select(\"value.HEAD\", \n",
    "                       \"value.FPGA\", \n",
    "                       \"value.TDC_CHANNEL\",\n",
    "                       \"value.ORBIT_CNT\",\n",
    "                       \"value.BX_COUNTER\",\n",
    "                       \"value.TDC_MEAS\")\n",
    "\n",
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee8ea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: string (nullable = true)\n",
      " |-- FPGA: string (nullable = true)\n",
      " |-- TDC_CHANNEL: string (nullable = true)\n",
      " |-- ORBIT_CNT: string (nullable = true)\n",
      " |-- BX_COUNTER: string (nullable = true)\n",
      " |-- TDC_MEAS: string (nullable = true)\n",
      " |-- CHAMBER: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data-cleansing\n",
    "df = flatDF.where(col(\"HEAD\")==2)\n",
    "\n",
    "# division of dataframe between chambers\n",
    "df = df.withColumn('CHAMBER',\n",
    "        when(( col( 'FPGA' ) == 0 ) & ( col( 'TDC_CHANNEL' ) >=  0 ) & ( col( 'TDC_CHANNEL' ) <  64  ), 0) \\\n",
    "       .when(( col( 'FPGA' ) == 0 ) & ( col( 'TDC_CHANNEL' ) >= 64 ) & ( col( 'TDC_CHANNEL' ) <= 127 ), 1) \\\n",
    "       .when(( col( 'FPGA' ) == 1 ) & ( col( 'TDC_CHANNEL' ) >=  0 ) & ( col( 'TDC_CHANNEL' ) <  64  ), 2) \\\n",
    "       .when(( col( 'FPGA' ) == 1 ) & ( col( 'TDC_CHANNEL' ) >= 64 ) & ( col( 'TDC_CHANNEL' ) <= 127 ), 3) \\\n",
    "       .when(( col( 'FPGA' ) == 1 ) &                                  ( col( 'TDC_CHANNEL' ) == 128 ), 4) \\\n",
    "               .otherwise(None))                                                                           \\\n",
    "      .filter( col( 'CHAMBER' ).isNotNull() )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scintillator time offset by Chamber\n",
    "# TODO compute ABSOLUTETIME and DRIFTIME\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "144ff0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_json = {\n",
    "    'epoch_id' : 0,\n",
    "    'hits'     : 0,\n",
    "    'CH0' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            }\n",
    "            },\n",
    "    'CH1' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            }\n",
    "            },\n",
    "    'CH2' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            }\n",
    "            },\n",
    "    'CH3' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                            }\n",
    "            },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47ddd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(df, epoch_id):   \n",
    "\n",
    "    '''\n",
    "    Function for the batch processing of the data.\n",
    "    The processing consists in retrieving\n",
    "    the following informations:\n",
    "        1. total number of processed hits, \n",
    "           post-clensing (1 value per batch)\n",
    "        2. total number of processed hits,\n",
    "           post-clensing, per chamber (4 values per batch)\n",
    "        3. histogram of the counts of active TDC_CHANNEL,\n",
    "           per chamber (4 arrays per batch)\n",
    "        4. histogram of the total number of active\n",
    "           TDC_CHANNEL in each ORBIT_CNT, per chamber \n",
    "           (4 arrays per batch)\n",
    "        5. histogram of the counts of active TDC_CHANNEL,\n",
    "           per chamber, ONLY for those orbits with at least\n",
    "           one scintillator signal in it (4 arrays per batch)\n",
    "        6. histogram of the DRIFTIME, per chamber \n",
    "           (4 arrays per batch) TODO\n",
    "    \n",
    "    Inputs:\n",
    "        - df: spark dataframe with the data\n",
    "        - epoch_id: batch index    \n",
    "    '''\n",
    "\n",
    "    msg_json['epoch_id']    = epoch_id\n",
    "\n",
    "    # total number of processed hits\n",
    "    # post-cleansing\n",
    "    total_hits = df.filter(col('CHAMBER') != 4)\\\n",
    "                   .count()\n",
    "\n",
    "\n",
    "    # total number of processed hits,\n",
    "    # post-clensing, per chamber\n",
    "    df_counts = df.filter(col('CHAMBER') != 4) \\\n",
    "                  .groupBy(   'CHAMBER')       \\\n",
    "                  .count()                     \\\n",
    "                  .withColumnRenamed('count', \n",
    "                                     'ch_hits')\n",
    "\n",
    "\n",
    "    # histogram of the counts of active\n",
    "    # TDC_CHANNEL, per chamber.\n",
    "    df_counts_ch = df.filter(col('CHAMBER') != 4)            \\\n",
    "                     .groupBy('TDC_CHANNEL', 'CHAMBER')      \\\n",
    "                     .count()                                \\\n",
    "                     .groupBy('CHAMBER')                     \\\n",
    "                     .agg(struct(collect_list('TDC_CHANNEL') \\\n",
    "                                       .alias(  'bin_edges'),\n",
    "                                 collect_list(      'count') \\\n",
    "                                       .alias(     'counts'),\n",
    "                                )                            \\\n",
    "                          .alias('histo_CH')\n",
    "                         )\n",
    "    \n",
    "    # histogram of the total number of active\n",
    "    # TDC_CHANNEL in each ORBIT_CNT per chamber\n",
    "    df_counts_orb = df.filter(col('CHAMBER') != 4)                    \\\n",
    "                      .groupBy('ORBIT_CNT', 'CHAMBER')                \\\n",
    "                      .agg(F.countDistinct(\"TDC_CHANNEL\"))            \\\n",
    "                      .groupBy('count(TDC_CHANNEL)', 'CHAMBER')       \\\n",
    "                      .count()                                        \\\n",
    "                      .groupBy('CHAMBER')                             \\\n",
    "                      .agg(struct(collect_list('count(TDC_CHANNEL)')  \\\n",
    "                                        .alias(         'bin_edges'),\n",
    "                                  collect_list(             'count') \\\n",
    "                                        .alias(            'counts'),\n",
    "                                 )                                    \\\n",
    "                           .alias('histo_ORB')\n",
    "                          )\n",
    "\n",
    "\n",
    "    # Histogram of the count of active TDC_CHANNEL,\n",
    "    # per chamber, only for those orbits with\n",
    "    # at least one scintillatorin it\n",
    "    \n",
    "    # revision needed: not sure about this\n",
    "    list_scint = df.filter(col('CHAMBER') == 4)   \\\n",
    "                   .select('ORBIT_CNT')           \\\n",
    "\n",
    "    df_counts_sc = df.filter(col('CHAMBER') != 4 )                  \\\n",
    "                     .join(list_scint, on=\"ORBIT_CNT\", how=\"inner\") \\\n",
    "                     .groupBy(  'TDC_CHANNEL', 'CHAMBER'       )    \\\n",
    "                     .count()                                       \\\n",
    "                     .groupBy('CHAMBER')                            \\\n",
    "                     .agg(struct(collect_list('TDC_CHANNEL')        \\\n",
    "                                         .alias('bin_edges'),\n",
    "                                 collect_list(      'count')        \\\n",
    "                                         .alias(   'counts'))       \\\n",
    "                          .alias('histo_SC')\n",
    "                         )\n",
    "\n",
    "    # dataframe with results joined\n",
    "    df_res = df_counts.join(df_counts_ch,  on='CHAMBER') \\\n",
    "                      .join(df_counts_orb, on='CHAMBER') \\\n",
    "                      .join(df_counts_sc,  on='CHAMBER') \\\n",
    "                      .sort('CHAMBER')                   \\\n",
    "                      .collect()\n",
    "                      \n",
    "    msg_json['hits'] = total_hits\n",
    "    for i in range(len(df_res)):\n",
    "        msg_json[f'CH{i}']['total_hits']               = df_res[i][  'ch_hits']\n",
    "        msg_json[f'CH{i}'][  'histo_CH'][ 'bin_edges'] = df_res[i][ 'histo_CH']['bin_edges']\n",
    "        msg_json[f'CH{i}'][  'histo_CH']['bin_counts'] = df_res[i][ 'histo_CH'][   'counts']\n",
    "        msg_json[f'CH{i}'][ 'histo_ORB'][ 'bin_edges'] = df_res[i]['histo_ORB']['bin_edges']\n",
    "        msg_json[f'CH{i}'][ 'histo_ORB']['bin_counts'] = df_res[i]['histo_ORB'][   'counts']\n",
    "        msg_json[f'CH{i}'][  'histo_SC'][ 'bin_edges'] = df_res[i][ 'histo_SC']['bin_edges']\n",
    "        msg_json[f'CH{i}'][  'histo_SC']['bin_counts'] = df_res[i][ 'histo_SC'][   'counts']\n",
    "\n",
    "    producer.send('topic_results', json.dumps(msg_json).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4de2c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d920458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 20:39:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-566d7b9c-dfd9-4801-ad3e-6249f4ce7d44. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/09/15 20:39:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                             00]]]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7efffbbef1f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1357, in <lambda>\n",
      "KeyboardInterrupt: \n",
      "                                                                                \n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.writeStream\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "545eda73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=0 host=pd-master:9092 <connecting> [IPv4 ('10.67.22.162', 9092)]> returned error 111. Disconnecting.\n",
      "WARNING:kafka.client:Node 0 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=0 host=pd-master:9092 <connecting> [IPv4 ('10.67.22.162', 9092)]> returned error 111. Disconnecting.\n",
      "WARNING:kafka.client:Node 0 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=0 host=pd-master:9092 <connecting> [IPv4 ('10.67.22.162', 9092)]> returned error 111. Disconnecting.\n",
      "WARNING:kafka.client:Node 0 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=0 host=pd-master:9092 <connecting> [IPv4 ('10.67.22.162', 9092)]> returned error 111. Disconnecting.\n",
      "WARNING:kafka.client:Node 0 connection failed -- refreshing metadata\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57ac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd66e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb67e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
