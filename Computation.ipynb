{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6165bdf4",
   "metadata": {},
   "source": [
    "# MAPD mod. B - Final Project\n",
    "##  Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a particle physics detector and publish the results in a\n",
    "dashboard for live monitoring.\n",
    "\n",
    "### Students:\n",
    "    - Aidin Attar - 2048654\n",
    "    - Ema Baci - 2050726\n",
    "    - Mariam Hergnyan - 2040478"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8673ad",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5716d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import findspark\n",
    "import pyspark\n",
    "import json\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "from kafka                import KafkaProducer\n",
    "from kafka.admin          import KafkaAdminClient, NewTopic\n",
    " \n",
    "from pyspark.sql           import functions as F\n",
    "from pyspark.sql           import SparkSession\n",
    "from pyspark.streaming     import StreamingContext\n",
    "from pyspark.sql.types     import StructField, StructType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, when, count, struct, collect_list\n",
    "from pyspark               import SparkConf, SparkContext\n",
    "from pyspark.sql.types     import StringType "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6badc",
   "metadata": {},
   "source": [
    "### Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0d2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize all the required variables\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7557507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aidin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aidin/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c2fe7915-89f5-432b-8a1e-63ce6008a512;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1042ms :: artifacts dl 39ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c2fe7915-89f5-432b-8a1e-63ce6008a512\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 19:13:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pd-master:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://pd-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_CosmicRays_Dashboard_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f87fa6cddc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start session - specify port, application name, and configuration settings.\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"spark://pd-master:7077\")\\\n",
    "        .appName(\"Project_CosmicRays_Dashboard_application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "        .config(\"spark.ui.port\", \"4041\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# default parallelism\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2e52c",
   "metadata": {},
   "source": [
    "### Kafka Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8523564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = 'pd-slave3:9092'\n",
    "\n",
    "producer    = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "kafka_admin = KafkaAdminClient(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d81a24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_results', 'topic_stream', '__consumer_offsets']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447ad826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input dataframe representing the stream of input lines from kafka\n",
    "# by connecting to the appropriate servers and topic\n",
    "\n",
    "inputDF = spark.readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'topic_stream')\\\n",
    "        .load()\n",
    "\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0fa0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the schema of the json data format used to create the messages\n",
    "\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        StringType()),\n",
    "                StructField(\"FPGA\",        StringType()),\n",
    "                StructField(\"TDC_CHANNEL\", StringType()),\n",
    "                StructField(\"ORBIT_CNT\",   StringType()),\n",
    "                StructField(\"BX_COUNTER\",  StringType()),\n",
    "                StructField(\"TDC_MEAS\",    StringType())\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408c15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: string (nullable = true)\n",
      " |    |-- FPGA: string (nullable = true)\n",
      " |    |-- TDC_CHANNEL: string (nullable = true)\n",
      " |    |-- ORBIT_CNT: string (nullable = true)\n",
      " |    |-- BX_COUNTER: string (nullable = true)\n",
      " |    |-- TDC_MEAS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a new DF is created from the previous by using the pyspark.sql functions\n",
    "\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))\n",
    "\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348f6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: string (nullable = true)\n",
      " |-- FPGA: string (nullable = true)\n",
      " |-- TDC_CHANNEL: string (nullable = true)\n",
      " |-- ORBIT_CNT: string (nullable = true)\n",
      " |-- BX_COUNTER: string (nullable = true)\n",
      " |-- TDC_MEAS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flatten the jsonDF into a proper DataFrame\n",
    "\n",
    "flatDF = jsonDF.select(\"value.HEAD\", \n",
    "                       \"value.FPGA\", \n",
    "                       \"value.TDC_CHANNEL\",\n",
    "                       \"value.ORBIT_CNT\",\n",
    "                       \"value.BX_COUNTER\",\n",
    "                       \"value.TDC_MEAS\")\n",
    "\n",
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee8ea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: string (nullable = true)\n",
      " |-- FPGA: string (nullable = true)\n",
      " |-- TDC_CHANNEL: string (nullable = true)\n",
      " |-- ORBIT_CNT: string (nullable = true)\n",
      " |-- BX_COUNTER: string (nullable = true)\n",
      " |-- TDC_MEAS: string (nullable = true)\n",
      " |-- CHAMBER: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data-cleansing\n",
    "df = flatDF.where(col(\"HEAD\")==2)\n",
    "\n",
    "# division of dataframe between chambers\n",
    "df = df.withColumn('CHAMBER',\n",
    "        when(( col( 'FPGA' ) == 0 ) & ( col( 'TDC_CHANNEL' ) >=  0 ) & ( col( 'TDC_CHANNEL' ) <  64  ), 0) \\\n",
    "       .when(( col( 'FPGA' ) == 0 ) & ( col( 'TDC_CHANNEL' ) >= 64 ) & ( col( 'TDC_CHANNEL' ) <= 127 ), 1) \\\n",
    "       .when(( col( 'FPGA' ) == 1 ) & ( col( 'TDC_CHANNEL' ) >=  0 ) & ( col( 'TDC_CHANNEL' ) <  64  ), 2) \\\n",
    "       .when(( col( 'FPGA' ) == 1 ) & ( col( 'TDC_CHANNEL' ) >= 64 ) & ( col( 'TDC_CHANNEL' ) <= 127 ), 3) \\\n",
    "       .when(( col( 'FPGA' ) == 1 ) &                                  ( col( 'TDC_CHANNEL' ) == 128 ), 4) \\\n",
    "       .otherwise(None))                                                                                   \\\n",
    "       .filter( col( 'CHAMBER' ).isNotNull() )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scintillator time offset by Chamber\n",
    "# TODO compute ABSOLUTETIME and DRIFTIME\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "144ff0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json structure for the message to be sent as result\n",
    "\n",
    "msg_json = {\n",
    "    'time'     : .0,\n",
    "    'epoch_id' :  0,\n",
    "    'hits'     :  0,\n",
    "    'CH0' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             }\n",
    "            },\n",
    "    'CH1' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             }\n",
    "            },\n",
    "    'CH2' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             }\n",
    "            },\n",
    "    'CH3' : {'total_hits'  : 0,\n",
    "             'histo_CH'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_ORB'   : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             },\n",
    "             'histo_SC'    : {'bin_edges'  : [],\n",
    "                              'bin_counts' : []\n",
    "                             }\n",
    "            },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ddd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(df, epoch_id):   \n",
    "\n",
    "    '''\n",
    "    Function for the batch processing of the data.\n",
    "    The processing consists in retrieving\n",
    "    the following informations:\n",
    "        1. total number of processed hits, \n",
    "           post-clensing (1 value per batch)\n",
    "        2. total number of processed hits,\n",
    "           post-clensing, per chamber (4 values per batch)\n",
    "        3. histogram of the counts of active TDC_CHANNEL,\n",
    "           per chamber (4 arrays per batch)\n",
    "        4. histogram of the total number of active\n",
    "           TDC_CHANNEL in each ORBIT_CNT, per chamber \n",
    "           (4 arrays per batch)\n",
    "        5. histogram of the counts of active TDC_CHANNEL,\n",
    "           per chamber, ONLY for those orbits with at least\n",
    "           one scintillator signal in it (4 arrays per batch)\n",
    "        6. histogram of the DRIFTIME, per chamber \n",
    "           (4 arrays per batch) TODO\n",
    "    \n",
    "    Inputs:\n",
    "        - df: spark dataframe with the data\n",
    "        - epoch_id: batch index    \n",
    "    '''\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    msg_json['epoch_id']    = epoch_id\n",
    "\n",
    "    # total number of processed hits\n",
    "    # post-cleansing\n",
    "    total_hits = df.filter(col('CHAMBER') != 4)\\\n",
    "                   .count()\n",
    "\n",
    "\n",
    "    # total number of processed hits,\n",
    "    # post-clensing, per chamber\n",
    "    df_counts = df.filter(col('CHAMBER') != 4) \\\n",
    "                  .groupBy(   'CHAMBER')       \\\n",
    "                  .count()                     \\\n",
    "                  .withColumnRenamed('count', \n",
    "                                     'ch_hits')\n",
    "\n",
    "\n",
    "    # histogram of the counts of active\n",
    "    # TDC_CHANNEL, per chamber.\n",
    "    df_counts_ch = df.filter(col('CHAMBER') != 4)            \\\n",
    "                     .groupBy('TDC_CHANNEL', 'CHAMBER')      \\\n",
    "                     .count()                                \\\n",
    "                     .groupBy('CHAMBER')                     \\\n",
    "                     .agg(struct(collect_list('TDC_CHANNEL') \\\n",
    "                                       .alias(  'bin_edges'),\n",
    "                                 collect_list(      'count') \\\n",
    "                                       .alias(     'counts'),\n",
    "                                )                            \\\n",
    "                          .alias('histo_CH')\n",
    "                         )\n",
    "    \n",
    "    # histogram of the total number of active\n",
    "    # TDC_CHANNEL in each ORBIT_CNT per chamber\n",
    "    df_counts_orb = df.filter(col('CHAMBER') != 4)                    \\\n",
    "                      .groupBy('ORBIT_CNT', 'CHAMBER')                \\\n",
    "                      .agg(F.countDistinct(\"TDC_CHANNEL\"))            \\\n",
    "                      .groupBy('count(TDC_CHANNEL)', 'CHAMBER')       \\\n",
    "                      .count()                                        \\\n",
    "                      .groupBy('CHAMBER')                             \\\n",
    "                      .agg(struct(collect_list('count(TDC_CHANNEL)')  \\\n",
    "                                        .alias(         'bin_edges'),\n",
    "                                  collect_list(             'count') \\\n",
    "                                        .alias(            'counts'),\n",
    "                                 )                                    \\\n",
    "                           .alias('histo_ORB')\n",
    "                          )\n",
    "\n",
    "\n",
    "    # Histogram of the count of active TDC_CHANNEL,\n",
    "    # per chamber, only for those orbits with\n",
    "    # at least one scintillator in it\n",
    "    \n",
    "    # revision needed: not sure about this\n",
    "    list_scint = df.filter(col('CHAMBER') == 4)   \\\n",
    "                   .select('ORBIT_CNT')           \\\n",
    "\n",
    "    df_counts_sc = df.filter(col('CHAMBER') != 4 )                  \\\n",
    "                     .join(list_scint, on=\"ORBIT_CNT\", how=\"inner\") \\\n",
    "                     .groupBy(  'TDC_CHANNEL', 'CHAMBER'       )    \\\n",
    "                     .count()                                       \\\n",
    "                     .groupBy('CHAMBER')                            \\\n",
    "                     .agg(struct(collect_list('TDC_CHANNEL')        \\\n",
    "                                         .alias('bin_edges'),\n",
    "                                 collect_list(      'count')        \\\n",
    "                                         .alias(   'counts'))       \\\n",
    "                          .alias('histo_SC')\n",
    "                         )\n",
    "\n",
    "    # dataframe with results joined\n",
    "    df_res = df_counts.join(df_counts_ch,  on='CHAMBER') \\\n",
    "                      .join(df_counts_orb, on='CHAMBER') \\\n",
    "                      .join(df_counts_sc,  on='CHAMBER') \\\n",
    "                      .sort('CHAMBER')                   \\\n",
    "                      .collect()\n",
    "                      \n",
    "    msg_json['hits'] = total_hits\n",
    "    for i in range(len(df_res)):\n",
    "        msg_json[f'CH{i}']['total_hits']               = df_res[i][  'ch_hits']\n",
    "        msg_json[f'CH{i}'][  'histo_CH'][ 'bin_edges'] = df_res[i][ 'histo_CH']['bin_edges']\n",
    "        msg_json[f'CH{i}'][  'histo_CH']['bin_counts'] = df_res[i][ 'histo_CH'][   'counts']\n",
    "        msg_json[f'CH{i}'][ 'histo_ORB'][ 'bin_edges'] = df_res[i]['histo_ORB']['bin_edges']\n",
    "        msg_json[f'CH{i}'][ 'histo_ORB']['bin_counts'] = df_res[i]['histo_ORB'][   'counts']\n",
    "        msg_json[f'CH{i}'][  'histo_SC'][ 'bin_edges'] = df_res[i][ 'histo_SC']['bin_edges']\n",
    "        msg_json[f'CH{i}'][  'histo_SC']['bin_counts'] = df_res[i][ 'histo_SC'][   'counts']\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # get the execution time\n",
    "    msg_json['time'] = end - start\n",
    "    \n",
    "    producer.send('topic_results', json.dumps(msg_json).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4de2c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d920458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 19:13:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ab5f6868-16a2-4696-bfbc-1aa36eaa0ed6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/09/19 19:13:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.200][Stage 9245:(161 + 4) / 200]]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7f87f84daca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1357, in <lambda>\n",
      "    lambda wr, cc=self._gateway_client, id=self._target_id:\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7f87fa730790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aidin/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1357, in <lambda>\n",
      "    lambda wr, cc=self._gateway_client, id=self._target_id:      (66 + 0) / 200]\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call the operations for each batch of data\n",
    "df.writeStream\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8b29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
