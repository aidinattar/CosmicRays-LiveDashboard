{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba336a1e",
   "metadata": {},
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5716d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries cell\n",
    "\n",
    "import json\n",
    "import time\n",
    "import findspark\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "from numpy import linspace \n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, max, min\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6badc",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e680ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls $SPARK_HOME/sbin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddba710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!$SPARK_HOME/sbin/stop-all.sh --host localhost --port 7077 --webui-port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00849269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!$SPARK_HOME/sbin/start-all.sh --host localhost --port 7077 --webui-port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c721d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0d2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of spark from the packages folder\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7557507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0dee6117-f97e-41a7-abe2-6791abbee659;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1031ms :: artifacts dl 46ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0dee6117-f97e-41a7-abe2-6791abbee659\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/23ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/11 14:12:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#start session - specify port, application name, and configuration settings.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Project_MAPDB_application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "        .config(\"spark.ui.port\", \"4041\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#default parallelism setting to shuffle different partitions between workers (for join operation).\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism) #15 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cfbe7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pd-master:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_MAPDB_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f30e54d4790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d04246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pd-master:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_MAPDB_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Project_MAPDB_application>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2e52c",
   "metadata": {},
   "source": [
    "## Kafka Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8523564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "925729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin = KafkaAdminClient(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea9335e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'topic_stream', 'results', '__consumer_offsets']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70898183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='stream', error_code=0), (topic='results', error_code=0)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.delete_topics(['stream', 'results'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ddad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'topic_stream', '__consumer_offsets']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d644147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_topic = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "kafka_admin.create_topics(new_topics=[results_topic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d81a24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'topic_stream', 'results', '__consumer_offsets']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "447ad826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the input dataframe and its source. Define subscription to topic_stream - one of the two topics in kafka\n",
    "inputDF = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'topic_stream')\\\n",
    "        .load()\n",
    "\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0fa0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the schema of the rows that will be read. double are used to overcome overflow issues\n",
    "schema = StructType(\n",
    "        [\n",
    "            StructField(\"HEAD\",        IntegerType()),\n",
    "            StructField(\"FPGA\",         IntegerType()),\n",
    "            StructField(\"TDC_CHANNEL\",  IntegerType()),\n",
    "            StructField(\"ORBIT_CNT\",    DoubleType()),\n",
    "            StructField(\"BX_COUNTER\",   DoubleType()), \n",
    "            StructField(\"TDC_MEAS\",    DoubleType() )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "408c15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: integer (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: double (nullable = true)\n",
      " |    |-- BX_COUNTER: double (nullable = true)\n",
      " |    |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert input_Df to json by casting columns into the predefined schema.\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))\n",
    "\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "348f6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: double (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#flattening the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")\n",
    "\n",
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47ddd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(df, epoch_id):   \n",
    "    \n",
    "    # total events\n",
    "    tot = df.count()\n",
    "    \n",
    "    # data-cleansing\n",
    "    df_clean = df.where(col('HEAD')== 2)\n",
    "\n",
    "    # repartition the df DataFrame to 105 parts - and persist in cache to speedup calculations\n",
    "    #df_clean.coalesce(15)\n",
    "    #df_clean.persist()\n",
    "    \n",
    "    # total number of processed hits, post-clensing\n",
    "    tot_hits = df_clean.count()\n",
    "\n",
    "\n",
    "    # division of the dataframe between chambers\n",
    "    df_ch0 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <=  63 )\n",
    "\n",
    "    df_ch1 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=  64 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <= 127 )\n",
    "\n",
    "    df_ch2 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   1 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <=  63 )\n",
    "\n",
    "    df_ch3 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   1 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=  64 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <= 127 )\n",
    "\n",
    "\n",
    "    # total number of processed hits,\n",
    "    #  post-clensing, per chamber\n",
    "    \n",
    "    tot_hits_ch0 = df_ch0.count()\n",
    "    tot_hits_ch1 = df_ch1.count()\n",
    "    tot_hits_ch2 = df_ch2.count()\n",
    "    tot_hits_ch3 = df_ch3.count()\n",
    "\n",
    "\n",
    "    # histogram of the counts of active\n",
    "    # TDC_CHANNEL, per chamber.\n",
    "\n",
    "    df0 = df_ch0                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "    df1 = df_ch1                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "    df2 = df_ch2                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "    df3 = df_ch3                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "\n",
    "    # histogram of the total number of active\n",
    "    # TDC_CHANNEL in each ORBIT_CNT per chamber\n",
    "    \n",
    "    df_orbs0 = df_ch0                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "\n",
    "    df_orbs1 = df_ch1                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "\n",
    "    df_orbs2 = df_ch2                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "    \n",
    "    df_orbs3 = df_ch3                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "\n",
    "\n",
    "    # Histogram of the count of active TDC_CHANNEL,\n",
    "    # per chamber, only for those orbits with\n",
    "    # at least one scintillatorin it\n",
    "\n",
    "    tdc128 = df_clean                        \\\n",
    "        .where(col(        'FPGA' ) ==   1 ) \\\n",
    "        .where(col( 'TDC_CHANNEL' ) == 128 ) \\\n",
    "        .toPandas()\n",
    "    l_orbs = tdc128['ORBIT_CNT'].tolist()\n",
    "    \n",
    "    scint_df_ch0 = df_ch0                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    scint_df_ch1 = df_ch1                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    scint_df_ch2 = df_ch2                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    scint_df_ch3 = df_ch3                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    \n",
    "    outputJson = \\\n",
    "        {\n",
    "            'tot_import':tot,\n",
    "            'hits': tot_hits,\n",
    "            'hitsPerChamber': [tot_hits_ch0, tot_hits_ch1, tot_hits_ch2, tot_hits_ch3],\n",
    "            'hist_ch0': [df0['TDC_CHANNEL'].tolist(), df0['count'].tolist()],\n",
    "            'hist_ch1': [df1['TDC_CHANNEL'].tolist(), df1['count'].tolist()],\n",
    "            'hist_ch2': [df2['TDC_CHANNEL'].tolist(), df2['count'].tolist()],\n",
    "            'hist_ch3': [df3['TDC_CHANNEL'].tolist(), df3['count'].tolist()],\n",
    "            'hist_orbit_ch0':[df_orbs0['count(TDC_CHANNEL)'].tolist(), df_orbs0['count'].tolist()],\n",
    "            'hist_orbit_ch1':[df_orbs1['count(TDC_CHANNEL)'].tolist(), df_orbs1['count'].tolist()],\n",
    "            'hist_orbit_ch2':[df_orbs2['count(TDC_CHANNEL)'].tolist(), df_orbs2['count'].tolist()],\n",
    "            'hist_orbit_ch3':[df_orbs3['count(TDC_CHANNEL)'].tolist(), df_orbs3['count'].tolist()],\n",
    "            'hist_scin_ch0': [scint_df_ch0['TDC_CHANNEL'].tolist(), scint_df_ch0['count'].tolist()],\n",
    "            'hist_scin_ch1': [scint_df_ch1['TDC_CHANNEL'].tolist(), scint_df_ch1['count'].tolist()],\n",
    "            'hist_scin_ch2': [scint_df_ch2['TDC_CHANNEL'].tolist(), scint_df_ch2['count'].tolist()],\n",
    "            'hist_scin_ch3': [scint_df_ch3['TDC_CHANNEL'].tolist(), scint_df_ch3['count'].tolist()],\n",
    "        }\n",
    "    \n",
    "    producer.send('results', json.dumps(outputJson).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4de2c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatDF.isStreaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d920458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/11 17:43:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6a66e9c5-4455-452e-bd38-48e6447d6e91. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/09/11 17:43:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flatDF.writeStream\\\n",
    "    .foreachBatch(analysis)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545eda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
