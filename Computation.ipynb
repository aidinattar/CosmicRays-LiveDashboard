{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba336a1e",
   "metadata": {},
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5716d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries cell\n",
    "\n",
    "import json\n",
    "import time\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    " \n",
    "from pyspark.sql           import functions as F\n",
    "from pyspark.sql           import SparkSession\n",
    "from pyspark.streaming     import StreamingContext\n",
    "from pyspark.sql.types     import StructField, StructType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark               import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6badc",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e680ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls $SPARK_HOME/sbin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddba710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd-slave2: no org.apache.spark.deploy.worker.Worker to stop\n",
      "pd-slave1: no org.apache.spark.deploy.worker.Worker to stop\n",
      "pd-master: no org.apache.spark.deploy.worker.Worker to stop\n",
      "no org.apache.spark.deploy.master.Master to stop\n"
     ]
    }
   ],
   "source": [
    "#!$SPARK_HOME/sbin/stop-all.sh --host localhost --port 7077 --webui-port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00849269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-mapd-b-gr05-1.out\n",
      "pd-slave2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr05-3.out\n",
      "pd-slave1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr05-4.out\n",
      "pd-master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr05-1.out\n"
     ]
    }
   ],
   "source": [
    "#!$SPARK_HOME/sbin/start-all.sh --host localhost --port 7077 --webui-port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c721d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0d2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of spark from the packages folder\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7557507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#start session - specify port, application name, and configuration settings.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Project_MAPDB_application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "        .config(\"spark.ui.port\", \"4041\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#default parallelism setting to shuffle different partitions between workers (for join operation).\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism) #15 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cfbe7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pd-master:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_MAPDB_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5d38109280>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d04246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pd-master:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_MAPDB_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Project_MAPDB_application>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2e52c",
   "metadata": {},
   "source": [
    "## Kafka Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8523564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin = KafkaAdminClient(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bea9335e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'topic_stream', 'results', '__consumer_offsets']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70898183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='results', error_code=0)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.delete_topics(['results'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1ddad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'topic_stream', '__consumer_offsets']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d644147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_topic = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "kafka_admin.create_topics(new_topics=[results_topic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d81a24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream', 'topic_stream', 'results', '__consumer_offsets']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "447ad826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the input dataframe and its source. Define subscription to 'stream' - one of the two topics in kafka\n",
    "inputDF = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'stream')\\\n",
    "        .load()\n",
    "\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0fa0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the schema of the rows that will be read. double are used to overcome overflow issues\n",
    "schema = StructType(\n",
    "        [\n",
    "            StructField(\"HEAD\",        IntegerType()),\n",
    "            StructField(\"FPGA\",        IntegerType()),\n",
    "            StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "            StructField(\"ORBIT_CNT\",    DoubleType()),\n",
    "            StructField(\"BX_COUNTER\",   DoubleType()), \n",
    "            StructField(\"TDC_MEAS\",     DoubleType())\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "408c15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: integer (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: double (nullable = true)\n",
      " |    |-- BX_COUNTER: double (nullable = true)\n",
      " |    |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert input_Df to json by casting columns into the predefined schema.\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))\n",
    "\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "348f6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: double (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#flattening the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")\n",
    "\n",
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47ddd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(df, epoch_id):   \n",
    "    \n",
    "    # total events\n",
    "    tot = df.count()\n",
    "    \n",
    "    # data-cleansing\n",
    "    df_clean = df.where(col('HEAD')== 2)\n",
    "\n",
    "    # repartition the df DataFrame to 105 parts - and persist in cache to speedup calculations\n",
    "    #df_clean.coalesce(15)\n",
    "    #df_clean.persist()\n",
    "    \n",
    "    # total number of processed hits, post-clensing\n",
    "    tot_hits = df_clean.count()\n",
    "\n",
    "\n",
    "    # division of the dataframe between chambers\n",
    "    df_ch0 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <=  63 )\n",
    "\n",
    "    df_ch1 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=  64 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <= 127 )\n",
    "\n",
    "    df_ch2 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   1 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=   0 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <=  63 )\n",
    "\n",
    "    df_ch3 = df_clean                         \\\n",
    "        .where( col(        'FPGA' ) ==   1 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) >=  64 ) \\\n",
    "        .where( col( 'TDC_CHANNEL' ) <= 127 )\n",
    "\n",
    "\n",
    "    # total number of processed hits,\n",
    "    #  post-clensing, per chamber\n",
    "    \n",
    "    tot_hits_ch0 = df_ch0.count()\n",
    "    tot_hits_ch1 = df_ch1.count()\n",
    "    tot_hits_ch2 = df_ch2.count()\n",
    "    tot_hits_ch3 = df_ch3.count()\n",
    "\n",
    "\n",
    "    # histogram of the counts of active\n",
    "    # TDC_CHANNEL, per chamber.\n",
    "\n",
    "    df0 = df_ch0                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "    df1 = df_ch1                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "    df2 = df_ch2                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "    df3 = df_ch3                \\\n",
    "        .groupBy('TDC_CHANNEL') \\\n",
    "        .count()                \\\n",
    "        .toPandas()\n",
    "\n",
    "\n",
    "    # histogram of the total number of active\n",
    "    # TDC_CHANNEL in each ORBIT_CNT per chamber\n",
    "    \n",
    "    df_orbs0 = df_ch0                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "\n",
    "    df_orbs1 = df_ch1                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "\n",
    "    df_orbs2 = df_ch2                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "    \n",
    "    df_orbs3 = df_ch3                        \\\n",
    "        .groupBy('ORBIT_CNT')                \\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')) \\\n",
    "        .groupBy(col('count(TDC_CHANNEL)'))  \\\n",
    "        .count()                             \\\n",
    "        .toPandas()\n",
    "\n",
    "\n",
    "    # Histogram of the count of active TDC_CHANNEL,\n",
    "    # per chamber, only for those orbits with\n",
    "    # at least one scintillatorin it\n",
    "\n",
    "    tdc128 = df_clean                        \\\n",
    "        .where(col(        'FPGA' ) ==   1 ) \\\n",
    "        .where(col( 'TDC_CHANNEL' ) == 128 ) \\\n",
    "        .toPandas()\n",
    "    l_orbs = tdc128['ORBIT_CNT'].tolist()\n",
    "    \n",
    "    scint_df_ch0 = df_ch0                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    scint_df_ch1 = df_ch1                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    scint_df_ch2 = df_ch2                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    scint_df_ch3 = df_ch3                           \\\n",
    "        .where( col( 'ORBIT_CNT' ).isin( l_orbs ) ) \\\n",
    "        .groupBy(  'TDC_CHANNEL'                  ) \\\n",
    "        .count()                                    \\\n",
    "        .toPandas()\n",
    "    \n",
    "    \n",
    "    results = \\\n",
    "        {\n",
    "            'tot_import':tot,\n",
    "            'hits': tot_hits,\n",
    "            'hitsPerChamber': [tot_hits_ch0, tot_hits_ch1, tot_hits_ch2, tot_hits_ch3],\n",
    "            'hist_ch0': [df0['TDC_CHANNEL'].tolist(), df0['count'].tolist()],\n",
    "            'hist_ch1': [df1['TDC_CHANNEL'].tolist(), df1['count'].tolist()],\n",
    "            'hist_ch2': [df2['TDC_CHANNEL'].tolist(), df2['count'].tolist()],\n",
    "            'hist_ch3': [df3['TDC_CHANNEL'].tolist(), df3['count'].tolist()],\n",
    "            'hist_orbit_ch0':[df_orbs0['count(TDC_CHANNEL)'].tolist(), df_orbs0['count'].tolist()],\n",
    "            'hist_orbit_ch1':[df_orbs1['count(TDC_CHANNEL)'].tolist(), df_orbs1['count'].tolist()],\n",
    "            'hist_orbit_ch2':[df_orbs2['count(TDC_CHANNEL)'].tolist(), df_orbs2['count'].tolist()],\n",
    "            'hist_orbit_ch3':[df_orbs3['count(TDC_CHANNEL)'].tolist(), df_orbs3['count'].tolist()],\n",
    "            'hist_scin_ch0': [scint_df_ch0['TDC_CHANNEL'].tolist(), scint_df_ch0['count'].tolist()],\n",
    "            'hist_scin_ch1': [scint_df_ch1['TDC_CHANNEL'].tolist(), scint_df_ch1['count'].tolist()],\n",
    "            'hist_scin_ch2': [scint_df_ch2['TDC_CHANNEL'].tolist(), scint_df_ch2['count'].tolist()],\n",
    "            'hist_scin_ch3': [scint_df_ch3['TDC_CHANNEL'].tolist(), scint_df_ch3['count'].tolist()],\n",
    "        }\n",
    "    \n",
    "    producer.send('results', json.dumps(results).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4de2c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatDF.isStreaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d920458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/12 14:56:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-88aca520-a570-4583-a454-6eaf6c38a382. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/09/12 14:56:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1714:>                                                       (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "flatDF.writeStream\\\n",
    "    .foreachBatch(analysis)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545eda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78e4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
